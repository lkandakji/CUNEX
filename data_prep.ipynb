{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF IMAGES ARE .WEBP RUN BELOW"
   ]
   "source": [
    "def convert_webp_to_png(directory):\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.webp'):\n",
    "                webp_path = os.path.join(subdir, file)\n",
    "                png_path = os.path.join(subdir, os.path.splitext(file)[0] + '.png')\n",
    "                \n",
    "                # Open the WEBP and convert it to PNG\n",
    "                image = Image.open(webp_path)\n",
    "                image.save(png_path, 'PNG')\n",
    "                print(f\"Converted {webp_path} to {png_path}\")\n",
    "\n",
    "                # Optionally, remove the original WEBP file\n",
    "                os.remove(webp_path)\n",
    "                print(f\"Removed original {webp_path}\")\n",
    "\n",
    "# Define your base directory\n",
    "base_dir = \"/home/lkandakji/image_analysis/segmentation/nnunet/all_annotations\"\n",
    "\n",
    "# Convert all WEBP images in the base directory\n",
    "convert_webp_to_png(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF IMAGES ARE .BMP RUN BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bmp_to_png(directory):\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.bmp'):\n",
    "                bmp_path = os.path.join(subdir, file)\n",
    "                png_path = os.path.join(subdir, os.path.splitext(file)[0] + '.png')\n",
    "                \n",
    "                # Open and convert the image\n",
    "                image = Image.open(bmp_path)\n",
    "                image.save(png_path, 'PNG')\n",
    "                print(f\"Converted {bmp_path} to {png_path}\")\n",
    "\n",
    "                # Optionally remove the original\n",
    "                os.remove(bmp_path)\n",
    "                print(f\"Removed original {bmp_path}\")\n",
    "\n",
    "# Define the two image directories\n",
    "dirs_to_convert = [\n",
    "    \"/home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Full-frame_Dataset/Original_AS-OCT_Images\",\n",
    "    \"/home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Partial-frame_Dataset/Original_AS-OCT_Images\"\n",
    "]\n",
    "\n",
    "# Run conversion on both\n",
    "for d in dirs_to_convert:\n",
    "    convert_bmp_to_png(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing directory: /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Full-frame_Dataset/Original_AS-OCT_Images or /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Full-frame_Dataset/annotations\n",
      "Missing directory: /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Partial-frame_Dataset/Original_AS-OCT_Images or /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Partial-frame_Dataset/annotations\n",
      "CSV file created at /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/image_label_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset\"\n",
    "csv_file_path = os.path.join(base_dir, \"image_label_mapping.csv\")\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['dataset_type', 'image_path', 'label_path'])\n",
    "\n",
    "    dataset_types = ['Full-frame_Dataset', 'Partial-frame_Dataset']\n",
    "    for dtype in dataset_types:\n",
    "        image_dir = os.path.join(base_dir, dtype, \"Original_AS-OCT_Images\")\n",
    "        label_dir = os.path.join(base_dir, dtype, \"annotations\")\n",
    "\n",
    "        if os.path.exists(image_dir) and os.path.exists(label_dir):\n",
    "            images = sorted(os.listdir(image_dir))\n",
    "            labels = sorted(os.listdir(label_dir))\n",
    "\n",
    "            for img, lbl in zip(images, labels):\n",
    "                if img == lbl:\n",
    "                    img_path = os.path.join(image_dir, img)\n",
    "                    lbl_path = os.path.join(label_dir, lbl)\n",
    "                    writer.writerow([dtype, img_path, lbl_path])\n",
    "                else:\n",
    "                    print(f\"[{dtype}] Mismatch: {img} â‰  {lbl}\")\n",
    "        else:\n",
    "            print(f\"Missing directory: {image_dir} or {label_dir}\")\n",
    "\n",
    "print(f\"CSV file created at {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/MS39_Raw/CSO2/stage1_segmentation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m existing_data[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     existing_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_data_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     12\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(file)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m~/image_analysis/nnunet_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/MS39_Raw/CSO2/stage1_segmentation.csv'"
     ]
    }
   ],
   "source": [
    "base_csv_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/all_annotations/image_label_mapping.csv\"\n",
    "new_data_csv_path = \"/mnt/data/MS39_Raw/CSO2/stage1_segmentation.csv\"\n",
    "\n",
    "with open(base_csv_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    existing_data = list(reader)\n",
    "\n",
    "if 'condition' not in existing_data[0]:\n",
    "    existing_data[0].append('condition')\n",
    "\n",
    "with open(new_data_csv_path, 'r', newline='') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if row['filename'] != \"OCT-0_23.png\":\n",
    "            new_row = [row['vol_id'], row['scan'], row['label'], \"keratoconus\"]\n",
    "            existing_data.append(new_row)\n",
    "\n",
    "with open(base_csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(existing_data)\n",
    "\n",
    "print(f\"CSV file updated and saved at {base_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated and saved at /home/lkandakji/image_analysis/segmentation/nnunet/all_annotations/image_label_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "new_data_csv_path = \"/mnt/data/MS39_Raw/CSO2/stage4_segmentation.csv\"\n",
    "\n",
    "with open(base_csv_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    existing_data = list(reader)\n",
    "\n",
    "with open(new_data_csv_path, 'r', newline='') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if row['filename'] != \"OCT-0_23.png\":\n",
    "            new_row = [row['vol_id'], row['scan'], row['label'], \"keratoconus\"]\n",
    "            existing_data.append(new_row)\n",
    "\n",
    "with open(base_csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(existing_data)\n",
    "\n",
    "print(f\"CSV file updated and saved at {base_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated and saved at /home/lkandakji/image_analysis/segmentation/nnunet/all_annotations/image_label_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "condition_files = {\n",
    "    \"fecd\": \"/mnt/scratch/mehphoenix/fecd_seg.csv\",\n",
    "    \"keratoconus\": \"/mnt/scratch/mehphoenix/kc_seg.csv\",\n",
    "    \"normal\": \"/mnt/scratch/mehphoenix/normals_seg.csv\"\n",
    "}\n",
    "\n",
    "patient_conditions = {}\n",
    "\n",
    "for condition, path in condition_files.items():\n",
    "    with open(path, 'r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            patient_id = row[-1].split('/')[-1]\n",
    "            patient_conditions[patient_id] = condition\n",
    "\n",
    "updated_data = []\n",
    "with open(base_csv_path, 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    headers = next(reader) \n",
    "    if 'condition' not in headers:\n",
    "        headers.append('condition') \n",
    "    updated_data.append(headers)\n",
    "\n",
    "    for row in reader:\n",
    "        patient_id = row[0]\n",
    "        if patient_id in patient_conditions:\n",
    "            if len(row) == len(headers):\n",
    "                row[-1] = patient_conditions[patient_id]\n",
    "            else:\n",
    "                row.append(patient_conditions[patient_id])\n",
    "        updated_data.append(row)\n",
    "\n",
    "with open(base_csv_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(updated_data)\n",
    "\n",
    "print(f\"CSV file updated and saved at {base_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_csv_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/all_annotations/image_label_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training, validation, and test sets successfully with no patient overlap.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1602177/3132287476.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data.drop('patient_condition', axis=1, inplace=True)\n",
      "/tmp/ipykernel_1602177/3132287476.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_data.drop('patient_condition', axis=1, inplace=True)\n",
      "/tmp/ipykernel_1602177/3132287476.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data.drop('patient_condition', axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(base_csv_path)\n",
    "train_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/train_kera.csv'\n",
    "val_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/val_kera.csv'\n",
    "test_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/test_kera.csv'\n",
    "\n",
    "data['patient_condition'] = data['patient'] + \"_\" + data['condition']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=42)\n",
    "\n",
    "for train_idx, test_val_idx in gss.split(data, groups=data['patient_condition']):\n",
    "    train_data = data.iloc[train_idx]\n",
    "    test_val_data = data.iloc[test_val_idx]\n",
    "\n",
    "gss_val_test = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for val_idx, test_idx in gss_val_test.split(test_val_data, groups=test_val_data['patient_condition']):\n",
    "    val_data = test_val_data.iloc[val_idx]\n",
    "    test_data = test_val_data.iloc[test_idx]\n",
    "\n",
    "train_data.drop('patient_condition', axis=1, inplace=True)\n",
    "val_data.drop('patient_condition', axis=1, inplace=True)\n",
    "test_data.drop('patient_condition', axis=1, inplace=True)\n",
    "\n",
    "train_data.to_csv(train_csv_path, index=False)\n",
    "val_data.to_csv(val_csv_path, index=False)\n",
    "test_data.to_csv(test_csv_path, index=False)\n",
    "\n",
    "print(\"Data split into training, validation, and test sets successfully with no patient overlap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 209 unique patients and condition\n",
      "fecd           71\n",
      "keratoconus    67\n",
      "normal         71\n",
      "Name: patient, dtype: int64.\n",
      "Validation set has 45 unique patients and condition\n",
      "fecd           16\n",
      "keratoconus    18\n",
      "normal         11\n",
      "Name: patient, dtype: int64.\n",
      "Test set has 45 unique patients and condition\n",
      "fecd           13\n",
      "keratoconus    15\n",
      "normal         17\n",
      "Name: patient, dtype: int64.\n"
     ]
    }
   ],
   "source": [
    "def print_unique_counts(csv_path, set_name):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    num_patients = data['patient'].nunique()\n",
    "    condition_counts = data.groupby('condition')['patient'].nunique()\n",
    "    print(f\"{set_name} set has {num_patients} unique patients and {condition_counts}.\")\n",
    "\n",
    "print_unique_counts(train_csv_path, \"Training\")\n",
    "print_unique_counts(val_csv_path, \"Validation\")\n",
    "print_unique_counts(test_csv_path, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at /home/lkandakji/image_analysis/segmentation/nnunet/sclera_lens_photos/scleral_photo_image_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"/home/lkandakji/image_analysis/segmentation/nnunet/sclera_lens_photos\"\n",
    "csv_file_path = os.path.join(image_dir, \"scleral_photo_image_mapping.csv\")\n",
    "\n",
    "image_filenames = [\"irregular_cornea.png\", \"regular_cornea.png\"]\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['dataset_type', 'image_path', 'label_path'])\n",
    "\n",
    "    for img in image_filenames:\n",
    "        img_path = os.path.join(image_dir, img)\n",
    "        if os.path.exists(img_path):\n",
    "            writer.writerow([\"scleral_lens_photo\", img_path, \"\"])\n",
    "        else:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "\n",
    "print(f\"CSV file created at {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN-UNET PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set test_out to True for testing with 100 pairs, False for full processing\n",
    "test_out = False\n",
    "NUM_TEST_OUT = 1000\n",
    "\n",
    "#REPLACE PATHS\n",
    "train_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/train_kera.csv'\n",
    "val_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/val_kera.csv'\n",
    "test_csv_path = '/home/lkandakji/image_analysis/segmentation/nnunet/test_kera.csv'\n",
    "output_base_path = '/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_kera'\n",
    "name_mapping_csv = '/home/lkandakji/image_analysis/segmentation/nnunet/nnunet_name_mapping_kera.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_external\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_scleral\"\n",
    "os.makedirs(os.path.join(output_base_path, 'imagesTs'), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs(os.path.join(output_base_path, 'imagesTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, 'labelsTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, 'imagesTs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, 'labelsTs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, 'visTr'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_base_path, 'visTs'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7e38d484-a2b7-43ca-a0f8-90f455c3266c</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>keratoconus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7e38d484-a2b7-43ca-a0f8-90f455c3266c</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>keratoconus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31ecd98a-767c-4a83-8c59-95057cd6f3b2</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>fecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31ecd98a-767c-4a83-8c59-95057cd6f3b2</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>fecd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d578e9ee-e3cb-4e04-a8e7-de20c2ba1b40</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>/home/lkandakji/image_analysis/segmentation/nn...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                patient  \\\n",
       "0  7e38d484-a2b7-43ca-a0f8-90f455c3266c   \n",
       "1  7e38d484-a2b7-43ca-a0f8-90f455c3266c   \n",
       "2  31ecd98a-767c-4a83-8c59-95057cd6f3b2   \n",
       "3  31ecd98a-767c-4a83-8c59-95057cd6f3b2   \n",
       "4  d578e9ee-e3cb-4e04-a8e7-de20c2ba1b40   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /home/lkandakji/image_analysis/segmentation/nn...   \n",
       "1  /home/lkandakji/image_analysis/segmentation/nn...   \n",
       "2  /home/lkandakji/image_analysis/segmentation/nn...   \n",
       "3  /home/lkandakji/image_analysis/segmentation/nn...   \n",
       "4  /home/lkandakji/image_analysis/segmentation/nn...   \n",
       "\n",
       "                                          label_path    condition  \n",
       "0  /home/lkandakji/image_analysis/segmentation/nn...  keratoconus  \n",
       "1  /home/lkandakji/image_analysis/segmentation/nn...  keratoconus  \n",
       "2  /home/lkandakji/image_analysis/segmentation/nn...         fecd  \n",
       "3  /home/lkandakji/image_analysis/segmentation/nn...         fecd  \n",
       "4  /home/lkandakji/image_analysis/segmentation/nn...       normal  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV files\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Combine train and validation data\n",
    "combined_df = pd.concat([train_df, val_df])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label_mapping = {'label_path': 1}\n",
    "\n",
    "def visualize_and_save(image_path, label_path, output_path):\n",
    "    # Load the image and label\n",
    "    image = Image.open(image_path)\n",
    "    label = Image.open(label_path)\n",
    "\n",
    "    # Convert label to RGB for visualization\n",
    "    label_array = np.array(label)\n",
    "    label_rgb = np.zeros((*label_array.shape, 3), dtype=np.uint8)  # Initialize RGB array\n",
    "\n",
    "    # Assign colors to each class (excluding background)\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]  # Red, Green, Blue, Yellow for 4 classes\n",
    "    for i, color in enumerate(colors, start=1):\n",
    "        label_rgb[label_array == i] = color\n",
    "\n",
    "    label_rgb_image = Image.fromarray(label_rgb)\n",
    "\n",
    "    # Overlay label on image\n",
    "    combined = Image.blend(image.convert('RGBA'), label_rgb_image.convert('RGBA'), alpha=0.5)\n",
    "\n",
    "    # Save the visualization\n",
    "    combined.save(output_path)\n",
    "\n",
    "# Function to process and copy images\n",
    "def process_and_copy(row, index, is_test=False):\n",
    "    try:\n",
    "        input_image_path = row['image_path']\n",
    "        pid = row['patient']\n",
    "        new_name = f'kera_{index:05d}'\n",
    "\n",
    "        # Open the input image and convert to grayscale if necessary\n",
    "        input_image = Image.open(input_image_path)\n",
    "        if input_image.mode != 'L':  # 'L' mode is for grayscale images\n",
    "            input_image = input_image.convert('L')\n",
    "\n",
    "        output_image_path = os.path.join(output_base_path, 'imagesTs' if is_test else 'imagesTr', new_name + '_0000.png')\n",
    "        input_image.save(output_image_path)\n",
    "\n",
    "        # Initialize label_image with the size of the input image\n",
    "        label_image = np.zeros(input_image.size, dtype=np.uint8)\n",
    "\n",
    "        for mask, label_index in label_mapping.items():\n",
    "            if row[mask] != 'blank':\n",
    "                annotation = Image.open(row[mask]).convert('L')  # Convert to grayscale\n",
    "                annotation_array = np.array(annotation)\n",
    "                \n",
    "                # Resize label_image if dimensions don't match\n",
    "                if annotation_array.shape != label_image.shape:\n",
    "                    label_image = np.zeros(annotation_array.shape, dtype=np.uint8)\n",
    "\n",
    "                label_image[annotation_array > 0] = label_index\n",
    "                \n",
    "        label_image_path = os.path.join(output_base_path, 'labelsTs' if is_test else 'labelsTr', new_name + '.png')\n",
    "        Image.fromarray(label_image).save(label_image_path)\n",
    "\n",
    "        # Visualization (for a subset of images)\n",
    "        if index < 10:\n",
    "            vis_output_path = os.path.join(output_base_path, 'visTs' if is_test else 'visTr', new_name + '.png')\n",
    "            visualize_and_save(output_image_path, label_image_path, vis_output_path)\n",
    "\n",
    "        return input_image_path, new_name\n",
    "\n",
    "    except (IOError, FileNotFoundError, UnidentifiedImageError) as e:\n",
    "        print(f\"Error processing file: {input_image_path}. Error: {e}. Skipping.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input directories\n",
    "input_dirs = [\n",
    "    \"/home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Full-frame_Dataset/Original_AS-OCT_Images\",\n",
    "    \"/home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Partial-frame_Dataset/Original_AS-OCT_Images\"\n",
    "]\n",
    "\n",
    "# Output directories\n",
    "output_base_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_external\"\n",
    "output_image_dir = os.path.join(output_base_path, \"imagesTs\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/home/lkandakji/image_analysis/segmentation/nnunet/sclera_lens_photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save(image_path, output_path):\n",
    "    image = Image.open(image_path).convert('L')  # Ensure grayscale\n",
    "    image.save(output_path)\n",
    "    \n",
    "def process_and_copy():\n",
    "    index = 0\n",
    "\n",
    "    for input_dir in input_dirs:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in sorted(files):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                    image_path = os.path.join(root, file)\n",
    "                    new_name = f'kera_{index:05d}_0000.png'\n",
    "                    output_image_path = os.path.join(output_image_dir, new_name)\n",
    "\n",
    "                    try:\n",
    "                        img = Image.open(image_path)\n",
    "                        if img.mode != 'L':\n",
    "                            img = img.convert('L')\n",
    "                        img.save(output_image_path)\n",
    "\n",
    "                    except (IOError, FileNotFoundError, UnidentifiedImageError) as e:\n",
    "                        print(f\"Error processing {image_path}: {e}\")\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m name_mapping \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39mcombined_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      5\u001b[0m     original_path, new_name \u001b[38;5;241m=\u001b[39m process_and_copy(row, index)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m original_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Process images\n",
    "name_mapping = []\n",
    "index = 0\n",
    "for _, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    original_path, new_name = process_and_copy(row, index)\n",
    "    if original_path is not None:\n",
    "        name_mapping.append([original_path, new_name])\n",
    "        index += 1\n",
    "        if test_out and index >= 100:\n",
    "            break\n",
    "\n",
    "# Process test images\n",
    "for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    original_path, new_name = process_and_copy(row, index, is_test=True)\n",
    "    name_mapping.append([original_path, new_name])\n",
    "    index += 1\n",
    "    if test_out and index >= NUM_TEST_OUT:\n",
    "        break\n",
    "\n",
    "# Save name mapping\n",
    "pd.DataFrame(name_mapping, columns=['OriginalPath', 'NNUNetName']).to_csv(name_mapping_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Full-frame_Dataset/Original_AS-OCT_Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 769/769 [01:43<00:00,  7.46it/s]\n",
      "Processing /home/lkandakji/image_analysis/segmentation/nnunet/AIDK_Dataset/Partial-frame_Dataset/Original_AS-OCT_Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 401/401 [00:53<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved 1168 images.\n",
      "Name mapping saved to: /home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_external/image_name_mapping.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name_mapping = []\n",
    "index = 0\n",
    "\n",
    "# Process all images\n",
    "for input_dir in input_dirs:\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in tqdm(sorted(files), desc=f\"Processing {input_dir}\"):\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                new_name = f'kera_{index:05d}_0000.png'\n",
    "                output_path = os.path.join(output_image_dir, new_name)\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(image_path)\n",
    "                    if img.mode != 'L':\n",
    "                        img = img.convert('L')\n",
    "                    img.save(output_path)\n",
    "\n",
    "                    name_mapping.append([image_path, new_name])\n",
    "                    index += 1\n",
    "\n",
    "                except (IOError, FileNotFoundError, UnidentifiedImageError) as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Save mapping CSV\n",
    "mapping_csv_path = os.path.join(os.path.dirname(output_image_dir), \"image_name_mapping.csv\")\n",
    "pd.DataFrame(name_mapping, columns=['OriginalPath', 'NNUNetName']).to_csv(mapping_csv_path, index=False)\n",
    "\n",
    "print(f\"Done. Saved {index} images.\")\n",
    "print(f\"Name mapping saved to: {mapping_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done. Saved 2 images.\n",
      "ðŸ“„ Name mapping saved to: /home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_scleral/image_name_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "input_dir = \"/home/lkandakji/image_analysis/segmentation/nnunet/sclera_lens_photos\"\n",
    "output_base_path = \"/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_raw/nnunet_raw_data/Dataset001_scleral\"\n",
    "output_image_dir = os.path.join(output_base_path, 'imagesTs')\n",
    "\n",
    "# Initialize\n",
    "image_files = [\"irregular_cornea.png\", \"regular_cornea.png\"]\n",
    "name_mapping = []\n",
    "\n",
    "# Process each image\n",
    "for index, file in enumerate(image_files):\n",
    "    image_path = os.path.join(input_dir, file)\n",
    "    new_name = f'kera_{index:05d}_0000.png'\n",
    "    output_path = os.path.join(output_image_dir, new_name)\n",
    "\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != 'L':\n",
    "            img = img.convert('L')\n",
    "        img.save(output_path)\n",
    "        name_mapping.append([image_path, new_name])\n",
    "\n",
    "    except (IOError, FileNotFoundError, UnidentifiedImageError) as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Save name mapping CSV\n",
    "mapping_csv_path = os.path.join(output_base_path, \"image_name_mapping.csv\")\n",
    "pd.DataFrame(name_mapping, columns=['OriginalPath', 'NNUNetName']).to_csv(mapping_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ… Done. Saved {len(name_mapping)} images.\")\n",
    "print(f\"ðŸ“„ Name mapping saved to: {mapping_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create dataset.json\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dataset_json \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0000\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m: index \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtest_out\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m NUM_TEST_OUT,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_ending\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_base_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(dataset_json, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_out' is not defined"
     ]
    }
   ],
   "source": [
    "# Create dataset.json\n",
    "dataset_json = {\n",
    "    \"channel_names\": {\"0\": \"0000\"},\n",
    "    \"labels\": {\"background\": 0, \"label_path\": 1},\n",
    "    \"numTraining\": index if not test_out else NUM_TEST_OUT,\n",
    "    \"file_ending\": \".png\"\n",
    "}\n",
    "with open(os.path.join(output_base_path, 'dataset.json'), 'w') as f:\n",
    "    json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset.json\n",
    "dataset_json = {\n",
    "    \"channel_names\": {\"0\": \"0000\"},\n",
    "    \"labels\": {},  # No training, so no labels\n",
    "    \"numTraining\": 0,\n",
    "    \"file_ending\": \".png\",\n",
    "}\n",
    "with open(os.path.join(output_base_path, 'dataset.json'), 'w') as f:\n",
    "    json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_identifiers(directory):\n",
    "    # Extracts file identifiers from the filenames (assumes format 'neo_{index}_0000.png')\n",
    "    files = os.listdir(directory)\n",
    "    identifiers = set(os.path.splitext(file)[0].split('_')[1] for file in files if file.endswith('.png'))\n",
    "    return identifiers\n",
    "\n",
    "# Get identifiers from imagesTr and labelsTr\n",
    "images_identifiers = get_file_identifiers(os.path.join(output_base_path, 'imagesTr'))\n",
    "labels_identifiers = get_file_identifiers(os.path.join(output_base_path, 'labelsTr'))\n",
    "\n",
    "# Ensure that each image has a corresponding label\n",
    "valid_identifiers = images_identifiers.intersection(labels_identifiers)\n",
    "\n",
    "# Convert to list and sort\n",
    "valid_identifiers = sorted(list(valid_identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold validation split considering patient IDs\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "splits = []\n",
    "\n",
    "# Use 'pid' column from combined_df as the groups\n",
    "for train_index, val_index in group_kfold.split(combined_df, groups=combined_df['patient']):\n",
    "    train_ids = [f'kera_{combined_df.iloc[i].name:05d}' for i in train_index]\n",
    "    val_ids = [f'kera_{combined_df.iloc[i].name:05d}' for i in val_index]\n",
    "    splits.append({'train': train_ids, 'val': val_ids})\n",
    "\n",
    "# Save splits\n",
    "with open('/home/lkandakji/image_analysis/segmentation/nnunet/dataset/nnunet_preprocessed/splits_final.json', 'w') as f:\n",
    "    json.dump(splits, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: change training cases value in dataset.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
